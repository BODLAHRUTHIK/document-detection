{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets                              ## importing inbuilt sklearn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np                                        ## importing numpy for easy mathematical calculations\n",
    "import re                                                 ## importing regex module for easy dealing with strings and substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups           ## fetching the inbult newsgroups dataset and getting\n",
    "train_data = fetch_20newsgroups(subset='train')           ## training and testing data seperately using subset parameter\n",
    "test_data = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data.data[0])                                 ## seeing how a document looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting all required training and testing split into numpy arrays\n",
    "x_train = np.array(train_data.data)\n",
    "y_train = np.array(train_data.target)\n",
    "x_test = np.array(test_data.data)\n",
    "y_test = np.array(test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  from nltk extracting stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating our dictionary using all words in train data \n",
    "## avoiding non alpha numneric characters, words whose length <= 2 and words in stop words \n",
    "d = {}\n",
    "for i in x_train:\n",
    "    x_row = i.lower()\n",
    "    x_row = re.split(r'\\W+', x_row)\n",
    "    for j in x_row:\n",
    "        if len(j)<=2 or not(j.isalpha()) or j in stop_words:\n",
    "            continue\n",
    "        d[j] = d.get(j, 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(e):\n",
    "    return e[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('edu', 21321),\n",
       " ('subject', 12264),\n",
       " ('com', 12133),\n",
       " ('lines', 11835),\n",
       " ('organization', 11233),\n",
       " ('one', 9008),\n",
       " ('would', 8905),\n",
       " ('writes', 7844),\n",
       " ('article', 7438),\n",
       " ('people', 5975)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sorting in descending order according to frequency of words\n",
    "d_s = sorted(d.items(), key = f, reverse = True)\n",
    "d_s[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## take out the most frequent words in training data\n",
    "## plotting graph to find how many number of features to take\n",
    "import matplotlib.pyplot as plt\n",
    "x_axis = []\n",
    "y_axis = []\n",
    "for i in range(len(d_s)):\n",
    "    x_axis.append(i)\n",
    "    y_axis.append(d_s[i][1])\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.axis([0, 8000, 1, 5000])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### by seeing above graph we decided to consider top 500 features for our  vocabulary\n",
    "### the accuracy would be more if we take around 2000 but it takes more time to run all those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['edu',\n",
       " 'subject',\n",
       " 'com',\n",
       " 'lines',\n",
       " 'organization',\n",
       " 'one',\n",
       " 'would',\n",
       " 'writes',\n",
       " 'article',\n",
       " 'people']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing the top 1000 frequent words in vocabulary\n",
    "vocabulary = [d_s[i][0] for i in range(1000)]\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating x_train_dataset\n",
    "## this is created like there will be columns of vocabulary and there will be rows of each document in training data \n",
    "## and values as number of times a particular word in vocabulary in that particular documnent\n",
    "x_train_dataset = np.zeros([len(x_train), len(vocabulary)], int)\n",
    "for i in range(len(x_train)):\n",
    "    words = re.split(r'[\\W]+', x_train[i].lower())\n",
    "    for j in words:\n",
    "        if j in vocabulary:\n",
    "            x_train_dataset[i][vocabulary.index(j)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating x_test_dataset\n",
    "## this is created like there will be columns of vocabulary and there will be rows of each document in testing data \n",
    "## and values as number of times a particular word in vocabulary in that particular documnent\n",
    "x_test_dataset = np.zeros([len(x_test), len(vocabulary)], int)\n",
    "for i in range(len(x_test)):\n",
    "    words = re.split(r'[\\W]+', x_test[i].lower())\n",
    "    for j in words:\n",
    "        if j in vocabulary:\n",
    "            x_test_dataset[i][vocabulary.index(j)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, ..., 0, 0, 0],\n",
       "       [3, 1, 0, ..., 0, 0, 0],\n",
       "       [2, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [6, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [3, 1, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the training data looks like this \n",
    "x_train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using sklearn inbuilt Multinomial naive bayes for the predictions \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_dataset, y_train)\n",
    "y_pred = clf.predict(x_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training data: 0.7761180837899947\n",
      "Score on testing data: 0.6408656399362719\n",
      "[[195   0   1   1   2   0   4   2  15   4   0   3   2   6   2  24   9   7\n",
      "    3  39]\n",
      " [  3 242   9  12  22  19  22   7   8   3   0   2  23   7   7   0   0   0\n",
      "    0   3]\n",
      " [  1  58 145  62  32  40  11   6   4   5   1   1  11  10   2   0   0   0\n",
      "    2   3]\n",
      " [  0  29  10 196  56   2  23   8   9   2   1   0  53   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0  11   2  48 240   0  31   8   7   3   1   0  26   3   4   1   0   0\n",
      "    0   0]\n",
      " [  0  76  16  13  16 221   9   4   5   4   0   3  16   3   8   0   0   0\n",
      "    1   0]\n",
      " [  0   3   0  19   9   0 330   8   4   1   2   1   9   1   1   0   0   0\n",
      "    1   1]\n",
      " [  1   2   3   4   5   1  21 287  28   2   1   0  20   6   7   0   2   0\n",
      "    5   1]\n",
      " [  1   2   0   3   0   1  11  24 329   2   1   0   5   6   4   0   3   0\n",
      "    4   2]\n",
      " [  3   4   0   0   2   0  18   4  17 292  38   0   7   6   2   0   3   0\n",
      "    1   0]\n",
      " [  0   2   0   1   2   0   4   2  15  58 306   0   1   0   3   0   0   0\n",
      "    3   2]\n",
      " [  0  14   2   0  10   2   5   8  13   0   0 286  18   8   6   0  11   1\n",
      "   10   2]\n",
      " [  1  27   0  29  32   3  18  22  17   4   2   8 207  12   3   0   6   0\n",
      "    0   2]\n",
      " [ 16  10   2   0  12   3  22  22  40   3   2   0  30 213   5   3   2   3\n",
      "    4   4]\n",
      " [  1  16   0   0   7   0   5  11  10   5   1   2  14  15 290   1   6   0\n",
      "    8   2]\n",
      " [ 21   5   1   0   2   2   3   1   8   3   2   0   3   3   4 307   2   2\n",
      "    0  29]\n",
      " [  7   0   0   2   1   0   5   9  26   2   0   5   4   9   1   0 261   3\n",
      "   18  11]\n",
      " [ 34   1   0   0   5   4   7   5   7   8   0   1   1   4   1   6  10 247\n",
      "   25  10]\n",
      " [  5   1   0   0   1   0   2   8  26   3   1   1   3  15  11   9  78   1\n",
      "  128  17]\n",
      " [ 46   4   0   0   1   0   2   4  12   2   2   0   1   5   2  33  22   4\n",
      "    6 105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.60       319\n",
      "           1       0.48      0.62      0.54       389\n",
      "           2       0.76      0.37      0.50       394\n",
      "           3       0.50      0.50      0.50       392\n",
      "           4       0.53      0.62      0.57       385\n",
      "           5       0.74      0.56      0.64       395\n",
      "           6       0.60      0.85      0.70       390\n",
      "           7       0.64      0.72      0.68       396\n",
      "           8       0.55      0.83      0.66       398\n",
      "           9       0.72      0.74      0.73       397\n",
      "          10       0.85      0.77      0.81       399\n",
      "          11       0.91      0.72      0.81       396\n",
      "          12       0.46      0.53      0.49       393\n",
      "          13       0.64      0.54      0.58       396\n",
      "          14       0.80      0.74      0.77       394\n",
      "          15       0.80      0.77      0.79       398\n",
      "          16       0.63      0.72      0.67       364\n",
      "          17       0.92      0.66      0.77       376\n",
      "          18       0.58      0.41      0.48       310\n",
      "          19       0.45      0.42      0.43       251\n",
      "\n",
      "    accuracy                           0.64      7532\n",
      "   macro avg       0.66      0.63      0.63      7532\n",
      "weighted avg       0.66      0.64      0.64      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## evaluating score and accuracy of the predictions\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Score on training data:\",clf.score(x_train_dataset,y_train))\n",
    "print(\"Score on testing data:\",clf.score(x_test_dataset,y_test))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing multinomial naive bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a fit function that returns a dictionary \n",
    "## consisting of total number of respective vocabulary words present in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train_dataset, y_train):\n",
    "    d = { }                                        ### creating a dictionary to generate vocabulary from all words present in our training data x_train\n",
    "    classes = set(y_train)                         ### listing out all possible classes available in training data\n",
    "    d['total_docs'] = len(y_train)\n",
    "    for i in classes:\n",
    "        d[i] = {}\n",
    "        d[i]['docs_in_class'] = len(y_train[y_train==i])\n",
    "        x_train_dataset_i = x_train_dataset[y_train==i]\n",
    "        total_class_words = 0\n",
    "        for j in vocabulary:\n",
    "            d[i][j] = (x_train_dataset_i[:, vocabulary.index(j)]).sum()\n",
    "            total_class_words += d[i][j]\n",
    "        d[i]['words_in_class'] = total_class_words\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicting class for a particular document and returning the predicted best fit class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_single(x, dictionary):\n",
    "    classes = set(dictionary.keys())\n",
    "    max_prob = -100000\n",
    "    best_fit_cls = 0\n",
    "    flag = True\n",
    "    for i in classes:\n",
    "        if i=='total_docs':\n",
    "            continue\n",
    "        product = np.log(dictionary[i]['docs_in_class']) - np.log(dictionary['total_docs'])\n",
    "        for j in vocabulary:\n",
    "            if x[vocabulary.index(j)]==0:\n",
    "                continue\n",
    "            l = len(vocabulary)\n",
    "            product += np.log(dictionary[i][j]+1) - np.log(dictionary[i]['words_in_class']+l)\n",
    "        if max_prob < product or flag:\n",
    "            max_prob = product\n",
    "            best_fit_cls = i\n",
    "            flag = False\n",
    "    return best_fit_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict function for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test_dataset, dictionary):\n",
    "    result = []\n",
    "    for i in x_test_dataset:\n",
    "        ans = predict_for_single(i, dictionary)\n",
    "        result.append(ans)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(x_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(x_test_dataset, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 0, 18, 0, 0, 8, 12, 5, 1]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normal score function defined to see how many of the actuall are predicted correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score(y_test, y_pred):\n",
    "    return (y_test==y_pred).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6295804567180032"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.63      0.60       319\n",
      "           1       0.43      0.63      0.51       389\n",
      "           2       0.70      0.17      0.27       394\n",
      "           3       0.54      0.51      0.52       392\n",
      "           4       0.53      0.66      0.59       385\n",
      "           5       0.75      0.51      0.61       395\n",
      "           6       0.60      0.86      0.71       390\n",
      "           7       0.62      0.74      0.68       396\n",
      "           8       0.55      0.84      0.67       398\n",
      "           9       0.77      0.75      0.76       397\n",
      "          10       0.89      0.79      0.84       399\n",
      "          11       0.91      0.72      0.80       396\n",
      "          12       0.40      0.54      0.46       393\n",
      "          13       0.62      0.51      0.56       396\n",
      "          14       0.81      0.66      0.73       394\n",
      "          15       0.82      0.76      0.79       398\n",
      "          16       0.62      0.73      0.67       364\n",
      "          17       0.92      0.64      0.75       376\n",
      "          18       0.53      0.40      0.46       310\n",
      "          19       0.45      0.41      0.43       251\n",
      "\n",
      "    accuracy                           0.63      7532\n",
      "   macro avg       0.65      0.62      0.62      7532\n",
      "weighted avg       0.66      0.63      0.63      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[202   1   0   2   1   1   4   3  14   3   0   2   3   7   4  16   8   7\n",
      "    3  38]\n",
      " [  2 244   5   8  22  16  17   8   9   1   1   3  36   8   7   0   0   0\n",
      "    1   1]\n",
      " [  3  89  66  81  38  43  14   4  10   3   1   3  19  13   1   0   1   0\n",
      "    3   2]\n",
      " [  0  31   8 200  62   1  22  12   5   1   0   0  49   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0  14   1  33 253   0  28   8   9   3   1   0  26   3   4   0   2   0\n",
      "    0   0]\n",
      " [  0  95  10  11  18 203  17   6   3   0   0   2  22   5   2   0   0   0\n",
      "    1   0]\n",
      " [  0   1   0  13  11   0 337   8   4   1   0   1  10   1   1   0   0   0\n",
      "    1   1]\n",
      " [  1   2   1   2   2   0  20 295  31   0   1   0  24   3   4   0   5   0\n",
      "    4   1]\n",
      " [  0   2   0   0   0   0  10  29 333   3   0   0   7   4   0   0   6   0\n",
      "    3   1]\n",
      " [  2   7   0   0   3   0  14   7  16 297  29   0  10   5   4   1   0   0\n",
      "    2   0]\n",
      " [  0   1   0   1   2   0   7   1  14  52 315   0   1   0   0   1   0   0\n",
      "    3   1]\n",
      " [  1  11   1   1   7   1   4   5  10   0   0 285  25   8   7   0  15   1\n",
      "   12   2]\n",
      " [  1  31   0  17  32   1  19  21  23   1   0   6 213  14   4   1   6   0\n",
      "    0   3]\n",
      " [ 20  10   1   0  11   1  19  30  38   3   1   0  41 202   6   2   2   2\n",
      "    4   3]\n",
      " [  7  18   0   0   5   0   7  13  15   3   0   2  25  16 262   2   4   1\n",
      "   10   4]\n",
      " [ 21   5   1   0   2   2   2   1   6   3   1   0   4   4   4 303   3   3\n",
      "    5  28]\n",
      " [  6   0   0   2   1   0   4   9  18   2   0   6   8   5   1   2 265   3\n",
      "   15  17]\n",
      " [ 35   1   0   0   6   0   5   7   8   3   2   1   3   2   3   6  13 240\n",
      "   34   7]\n",
      " [ 10   1   0   0   1   0   2   7  24   3   1   2   5  18   8   8  76   1\n",
      "  125  18]\n",
      " [ 48   4   0   0   0   0   6   2  11   3   1   0   1   7   3  29  21   4\n",
      "    9 102]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on testing data from scratch code: 0.6295804567180032\n"
     ]
    }
   ],
   "source": [
    "print(\"Score on testing data from scratch code:\",score(y_pred, y_test))    ### calculating score using code from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
